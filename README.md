# a-pertubation-in-jailbreak-space
In this work, we investigate the internal representations of language models during Best-of-N(BoN) jailbreak attacksâ€”an adversarial prompting technique introduced by Anthropic. Specifically, we conduct an activation-level analysis of the LLaMA architecture to uncover how the model internally encodes prompt perturbations that lead to successful jailbreaks. Our method involves performing Principal Component Analysis (PCA) on hidden activations across various layers for sequences of incrementally perturbed prompts, ranging from the original to a successful N-th jailbreak. By visualizing these activations, we identify a specific layer that exhibits a strong, consistent trajectory reflecting the progression of perturbations. This directional consistency becomes most apparent when analyzing triplets of prompts (three different harmful requests), suggesting that certain layers may encode a latent "perturbation direction" in representation space. These findings raise the possibility of identifying and potentially intercepting adversarial manipulations through activation-based monitoring.
Future work will explore whether these patterns persist across different model architectures, broader sets of prompts, and other modalities such as audio or image-based inputs, domains where best-of-N strategies have also worked well. Additionally, extending the analysis to more layers may reveal richer dynamics and allow for better interpretability of how jailbreaks are encoded and executed internally. This study provides a preliminary but promising step toward mechanistically understanding adversarial robustness and the internal geometry of model vulnerabilities.

Keywords: BoN jailbreaks, perturbation directions, activation analysis, interpretability
